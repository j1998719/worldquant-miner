"""
run_scrawler.py
åˆä½µçˆ¬èŸ²å’Œé è™•ç†åŠŸèƒ½ï¼Œä½¿ç”¨æœ¬åœ° Ollama gemma3:1b æ¨¡å‹
"""

import time
import datetime
import json
import unicodedata
import requests
from pathlib import Path
from bs4 import BeautifulSoup
from loguru import logger
from playwright.sync_api import sync_playwright

from utils.config_loader import ConfigLoader

# ============================================================================
# é…ç½®
# ============================================================================
OLLAMA_URL = "http://localhost:11434"
OLLAMA_MODEL = "gemma3:1b"

# ç›®éŒ„è¨­ç½®
BASE_DIR = Path(__file__).resolve().parents[1]
RAW_DIR = BASE_DIR / "data" / "wq_posts" / "raw_posts"
RAW_DIR.mkdir(parents=True, exist_ok=True)
PROCESSED_DIR = BASE_DIR / "data" / "wq_posts" / "processed_posts"
PROCESSED_DIR.mkdir(parents=True, exist_ok=True)
HELPFUL_DIR = BASE_DIR / "data" / "wq_posts" / "helpful_posts"
HELPFUL_DIR.mkdir(parents=True, exist_ok=True)

COOKIES_FILE = RAW_DIR / "cookies.json"


# ============================================================================
# æ–‡æœ¬æ¸…ç†å’Œæå–
# ============================================================================

def clean_text(text: str) -> str:
    """æ¸…æ´—æ–‡æœ¬ä¸­éutf-8å­—ç¬¦ï¼Œçµ±ä¸€ç‚º NFC æ ¼å¼"""
    if not text:
        return ""
    cleaned = text.encode("utf-8", errors="ignore").decode("utf-8", errors="ignore")
    cleaned = unicodedata.normalize("NFC", cleaned)
    return cleaned.strip()


def extract_post_info(html_content: str) -> dict:
    """å¾å–®å€‹HTMLä¸­æŠ½å– description, title, post-body, post-comments"""
    soup = BeautifulSoup(html_content, "html.parser")

    # description
    description = ""
    meta_desc = soup.find("meta", attrs={"name": "description"})
    if meta_desc and meta_desc.get("content"):
        description = meta_desc.get("content").strip()
    if not description:
        og_desc = soup.find("meta", property="og:description")
        if og_desc and og_desc.get("content"):
            description = og_desc.get("content").strip()

    # title
    title = ""
    og_title = soup.find("meta", property="og:title")
    if og_title and og_title.get("content"):
        title = og_title.get("content").strip()
    if not title and soup.title:
        title = soup.title.string.strip()

    # post-body
    post_body = ""
    body_div = soup.find("div", class_="post-body")
    if body_div:
        post_body = body_div.get_text("\n", strip=True)

    # comments
    comments = []
    for section in soup.select("section.comment-body"):
        text = section.get_text("\n", strip=True)
        if text:
            comments.append(text)

    return {
        "title": title,
        "description": description,
        "post_body": post_body,
        "post_comments": comments,
    }



def build_check_if_blog_helpful(post_file):
    """
    æ§‹å»ºç”¨æ–¼åˆ¤æ–·å¸–å­æ˜¯å¦æœ‰å¹«åŠ©çš„ promptï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼Œé¡ä¼¼ adaptive_alpha_minerï¼‰
    
    Args:
        post_file: è™•ç†å¾Œçš„ JSON æ–‡ä»¶è·¯å¾‘
    
    Returns:
        str: æ ¼å¼åŒ–çš„ prompt å­—ç¬¦ä¸²
    """
    # è®€å–å¸–å­å…§å®¹
    with open(post_file, 'r', encoding='utf-8') as f:
        post_data = json.load(f)
    
    title = post_data.get('title', '')
    description = post_data.get('description', '')
    post_body = post_data.get('post_body', '')
    
    # æ§‹å»ºå–®å€‹å­—ç¬¦ä¸² promptï¼ˆåƒè€ƒ adaptive_alpha_miner çš„é¢¨æ ¼ï¼‰
    prompt = f"""You are an expert in quantitative finance and alpha factor research.
Analyze the following WorldQuant community post and determine if it contains useful information for alpha factor mining.

A helpful post should contain:
- Specific alpha factor ideas or expressions
- Technical discussions about factor construction
- Mathematical formulas or data operations
- Trading strategies or signal generation methods
- Performance analysis or backtesting insights
- Code examples or implementation details

A post is NOT helpful if it only contains:
- General questions without technical content
- Administrative/account issues
- Simple greetings or thank you messages
- Off-topic discussions

POST TITLE:
{title}

POST DESCRIPTION:
{description}

POST BODY:
{post_body[:2000]}

TASK:
Based on the above content, is this post helpful for alpha factor research?
Answer with ONLY ONE WORD: "YES" or "NO"

ANSWER:"""

    return prompt



# ============================================================================
# Ollama LLM èª¿ç”¨
# ============================================================================

def call_ollama_llm(prompt: str, model: str = OLLAMA_MODEL, timeout: int = 60) -> str:
    """
    èª¿ç”¨æœ¬åœ° Ollama æ¨¡å‹
    
    Args:
        prompt: æç¤ºè©
        model: æ¨¡å‹åç¨±
        timeout: è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰
    
    Returns:
        æ¨¡å‹çš„éŸ¿æ‡‰æ–‡æœ¬
    """
    url = f"{OLLAMA_URL}/api/generate"
    
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": 0.2,
            "num_predict": 100  # é™åˆ¶è¼¸å‡ºé•·åº¦
        }
    }
    
    try:
        logger.info(f"ğŸ¤– èª¿ç”¨ Ollama æ¨¡å‹: {model}")
        start_time = time.time()
        
        response = requests.post(url, json=payload, timeout=timeout)
        
        elapsed_time = time.time() - start_time
        logger.info(f"â±ï¸  Ollama éŸ¿æ‡‰æ™‚é–“: {elapsed_time:.2f}s")
        
        if response.status_code == 200:
            result = response.json()
            response_text = result.get('response', '').strip()
            logger.info(f"ğŸ’¬ Ollama å›æ‡‰: {response_text[:100]}...")
            return response_text
        else:
            logger.error(f"âŒ Ollama API éŒ¯èª¤: {response.status_code}")
            logger.error(f"   {response.text[:200]}")
            return ""
            
    except requests.exceptions.Timeout:
        logger.error(f"â±ï¸  Ollama è«‹æ±‚è¶…æ™‚ï¼ˆ{timeout}sï¼‰")
        return ""
    except Exception as e:
        logger.error(f"âŒ èª¿ç”¨ Ollama å¤±æ•—: {e}")
        return ""


def check_if_post_helpful(post_file: Path) -> bool:
    """
    ä½¿ç”¨ Ollama æª¢æŸ¥å¸–å­æ˜¯å¦æœ‰å¹«åŠ©
    
    Args:
        post_file: è™•ç†å¾Œçš„ JSON æ–‡ä»¶è·¯å¾‘
    
    Returns:
        True å¦‚æœæœ‰å¹«åŠ©ï¼ŒFalse å¦å‰‡
    """
    try:
        # æ§‹å»º promptï¼ˆç›´æ¥è¿”å›å­—ç¬¦ä¸²ï¼‰
        prompt = build_check_if_blog_helpful(post_file)
        
        # èª¿ç”¨ Ollama
        answer = call_ollama_llm(prompt, model=OLLAMA_MODEL)
        
        if not answer:
            logger.warning(f"âš ï¸  Ollama è¿”å›ç©ºéŸ¿æ‡‰")
            return False
        
        logger.info(f"ğŸ” åˆ¤æ–· {post_file.name} æ˜¯å¦æœ‰å¹«åŠ©: {answer}")
        
        # åˆ¤æ–·å›ç­”
        if answer.upper().startswith("Y"):
            logger.info(f"âœ… å¸–å­è¢«åˆ¤å®šç‚ºæœ‰å¹«åŠ©")
            return True
        else:
            logger.info(f"âŒ å¸–å­è¢«åˆ¤å®šç‚ºç„¡å¹«åŠ©")
            return False
            
    except Exception as e:
        logger.error(f"âš ï¸  check_if_post_helpful éŒ¯èª¤: {e}")
        return False


# ============================================================================
# çˆ¬èŸ²ç›¸é—œå‡½æ•¸
# ============================================================================


def _save_raw_html(post_id: str, html_content: str):
    """ä¿å­˜åŸå§‹HTMLæ–‡ä»¶"""
    now_str = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    file_path = RAW_DIR / f"{now_str}_{post_id}.html"
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(html_content)
    logger.info(f"ğŸ’¾ ä¿å­˜åŸå§‹ HTML åˆ° {file_path}")
    return file_path


def process_html_to_json(html_file: Path, post_id: str) -> Path:
    """
    è™•ç†å–®å€‹ HTML æ–‡ä»¶ç‚º JSON
    
    Args:
        html_file: HTML æ–‡ä»¶è·¯å¾‘
        post_id: å¸–å­ ID
    
    Returns:
        è™•ç†å¾Œçš„ JSON æ–‡ä»¶è·¯å¾‘
    """
    out_file = PROCESSED_DIR / f"{html_file.stem}.json"
    
    logger.info(f"ğŸ“ è™•ç† {html_file.name}...")
    html_content = html_file.read_text(encoding="utf-8", errors="ignore")
    html_content = clean_text(html_content)
    
    post_info = extract_post_info(html_content)
    
    with open(out_file, "w", encoding="utf-8") as f:
        json.dump(post_info, f, ensure_ascii=False, indent=2)
    
    logger.info(f"âœ… ä¿å­˜è™•ç†å¾Œçš„ JSON åˆ° {out_file}")
    return out_file


def scrape_new_posts(limit: int = 50, auto_process: bool = True):
    """
    Playwright æŠ“å– WorldQuant Consultant æ–°å¸–å­
    
    Args:
        limit: æŠ“å–å¸–å­æ•¸é‡ä¸Šé™ï¼ˆé»˜èª 50ï¼‰
        auto_process: æ˜¯å¦è‡ªå‹•è™•ç†ä¸¦åˆ¤æ–·æ˜¯å¦æœ‰å¹«åŠ©
    
    Returns:
        æ–°å¸–å­å…ƒæ•¸æ“šåˆ—è¡¨
    """
    topic_url = ConfigLoader.get("worldquant_consultant_posts_url")
    new_posts_meta = []

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        context = browser.new_context()
        page = context.new_page()

        logger.info(f"ğŸŒ å°èˆªåˆ°ä¸»é¡Œé é¢: {topic_url}")
        page.goto(topic_url)

        logger.info("â¸ï¸  è«‹åœ¨ç€è¦½å™¨ä¸­å®Œæˆç™»éŒ„...")
        input("âœ… å·²ç™»éŒ„ä¸¦çœ‹åˆ°å¸–å­åˆ—è¡¨å¾ŒæŒ‰å›è»Šç¹¼çºŒ...")

        fetched_count = 0
        has_next = True
        
        while has_next and fetched_count < limit:
            # ç­‰å¾…é é¢åŠ è¼‰
            try:
                page.wait_for_load_state("load", timeout=60000)
            except:
                logger.warning("âš ï¸  load_state timeout, continue anyway")
            time.sleep(5)

            html = page.content()
            soup = BeautifulSoup(html, "html.parser")

            # æŠ½å–å¸–å­éˆæ¥
            post_links = soup.select("a[href*='/community/posts/']")
            logger.info(f"ğŸ“‹ æ‰¾åˆ° {len(post_links)} å€‹å¸–å­éˆæ¥")

            for link in post_links:
                if fetched_count >= limit:
                    break
                    
                post_url = link.get("href")
                if not post_url:
                    continue
                    
                full_url = (
                    post_url
                    if post_url.startswith("http")
                    else "https://support.worldquantbrain.com" + post_url
                )

                import re
                m = re.search(r"/posts/(\d+)", full_url)
                if not m:
                    continue
                post_id = m.group(1)

                # æŠ“å–å¸–å­è©³æƒ…é 
                logger.info(f"\n{'='*60}")
                logger.info(f"ğŸ“„ æŠ“å–å¸–å­ {fetched_count + 1}/{limit}: ID={post_id}")
                logger.info(f"{'='*60}")
                
                page.goto(full_url)
                try:
                    page.wait_for_load_state("load", timeout=60000)
                except:
                    logger.warning("âš ï¸  Timeout loading post page")
                time.sleep(3)
                
                html_content = page.content()
                html_file = _save_raw_html(post_id, html_content)

                title = link.get_text(strip=True)
                post_meta = {
                    "id": post_id,
                    "title": title,
                    "url": full_url,
                    "time": "",
                }
                new_posts_meta.append(post_meta)
                fetched_count += 1

                # è‡ªå‹•è™•ç†
                if auto_process:
                    logger.info(f"\nğŸ”„ é–‹å§‹è™•ç†å¸–å­ {post_id}...")
                    json_file = process_html_to_json(html_file, post_id)
                    
                    # åˆ¤æ–·æ˜¯å¦æœ‰å¹«åŠ©
                    logger.info(f"\nğŸ¤– ä½¿ç”¨ Ollama åˆ¤æ–·å¸–å­æ˜¯å¦æœ‰å¹«åŠ©...")
                    if check_if_post_helpful(json_file):
                        helpful_file = HELPFUL_DIR / f"{json_file.name}"
                        
                        with open(json_file, 'r', encoding='utf-8') as f:
                            post_info = json.load(f)
                        
                        with open(helpful_file, "w", encoding="utf-8") as f:
                            json.dump(post_info, f, ensure_ascii=False, indent=2)
                        
                        logger.info(f"â­ æœ‰å¹«åŠ©çš„å¸–å­å·²ä¿å­˜åˆ° {helpful_file}")
                    else:
                        logger.info(f"ğŸ“ å¸–å­è™•ç†å®Œæˆä½†æœªæ¨™è¨˜ç‚ºæœ‰å¹«åŠ©")

                # å›åˆ°åˆ—è¡¨é 
                page.goto(topic_url)
                try:
                    page.wait_for_load_state("load", timeout=60000)
                except:
                    logger.warning("âš ï¸  Timeout loading topic page")
                time.sleep(5)

            # ç¿»é 
            next_button = page.locator("a:has-text('Next')")
            if next_button.count() > 0 and fetched_count < limit:
                logger.info("â¡ï¸  é»æ“Š Next æŒ‰éˆ•...")
                next_button.first.click()
                try:
                    page.wait_for_load_state("load", timeout=60000)
                except:
                    logger.warning("âš ï¸  Timeout after clicking Next")
                time.sleep(5)
            else:
                has_next = False

        # ä¿å­˜ cookies
        context.storage_state(path=str(COOKIES_FILE))
        logger.info(f"ğŸª ä¿å­˜ cookies åˆ° {COOKIES_FILE}")
        browser.close()

    logger.info(f"\n{'='*60}")
    logger.info(f"âœ… çˆ¬å–å®Œæˆï¼ç¸½å…±æŠ“å– {len(new_posts_meta)} å€‹æ–°å¸–å­")
    logger.info(f"{'='*60}")
    return new_posts_meta


def preprocess_existing_posts():
    """æ‰¹é‡è™•ç†å·²æœ‰çš„æœªè™•ç† HTML æ–‡ä»¶"""
    raw_files = list(RAW_DIR.glob("*.html"))
    logger.info(f"\nğŸ“‚ æ‰¾åˆ° {len(raw_files)} å€‹åŸå§‹ HTML æ–‡ä»¶")
    
    processed_count = 0
    helpful_count = 0

    for raw_file in raw_files:
        post_id = raw_file.stem
        out_file = PROCESSED_DIR / f"{post_id}.json"
        
        if out_file.exists():
            logger.info(f"â­ï¸  è·³éå·²è™•ç†: {raw_file.name}")
            continue

        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“ è™•ç†: {raw_file.name}")
        
        out_file = process_html_to_json(raw_file, post_id)
        processed_count += 1
        
        # åˆ¤æ–·æ˜¯å¦æœ‰å¹«åŠ©
        if check_if_post_helpful(out_file):
            helpful_file = HELPFUL_DIR / f"{post_id}.json"
            
            with open(out_file, 'r', encoding='utf-8') as f:
                post_info = json.load(f)
            
            with open(helpful_file, "w", encoding="utf-8") as f:
                json.dump(post_info, f, ensure_ascii=False, indent=2)
            
            helpful_count += 1
            logger.info(f"â­ æœ‰å¹«åŠ©çš„å¸–å­å·²ä¿å­˜")

    logger.info(f"\n{'='*60}")
    logger.info(f"âœ… è™•ç†å®Œæˆï¼")
    logger.info(f"   æ–°è™•ç†: {processed_count} å€‹æ–‡ä»¶")
    logger.info(f"   æœ‰å¹«åŠ©: {helpful_count} å€‹å¸–å­")
    logger.info(f"{'='*60}")


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='WorldQuant å¸–å­çˆ¬èŸ²å’Œè™•ç†å·¥å…·ï¼ˆä½¿ç”¨ Ollamaï¼‰')
    parser.add_argument('--mode', type=str, 
                       choices=['scrape', 'process', 'both'],
                       default='both',
                       help='é‹è¡Œæ¨¡å¼: scrape(åƒ…çˆ¬å–), process(åƒ…è™•ç†), both(çˆ¬å–ä¸¦è™•ç†)')
    parser.add_argument('--limit', type=int, default=50,
                       help='æŠ“å–å¸–å­æ•¸é‡ä¸Šé™ï¼ˆé»˜èª: 50ï¼‰')
    parser.add_argument('--model', type=str, default='gemma3:1b',
                       help='Ollama æ¨¡å‹åç¨±ï¼ˆé»˜èª: gemma3:1bï¼‰')
    
    args = parser.parse_args()
    
    # è¨­ç½®å…¨å±€æ¨¡å‹
    global OLLAMA_MODEL
    OLLAMA_MODEL = args.model
    
    logger.info(f"\n{'='*80}")
    logger.info(f"WorldQuant å¸–å­çˆ¬èŸ²å’Œè™•ç†å·¥å…·")
    logger.info(f"ä½¿ç”¨ Ollama æ¨¡å‹: {OLLAMA_MODEL}")
    logger.info(f"{'='*80}\n")
    
    if args.mode in ['scrape', 'both']:
        logger.info("ğŸš€ é–‹å§‹çˆ¬å–æ–°å¸–å­...")
        new_posts = scrape_new_posts(limit=args.limit, auto_process=True)
        logger.info(f"âœ… çˆ¬å–äº† {len(new_posts)} å€‹æ–°å¸–å­")
    
    if args.mode == 'process':
        logger.info("ğŸ”„ è™•ç†ç¾æœ‰æœªè™•ç†çš„å¸–å­...")
        preprocess_existing_posts()
    
    logger.info(f"\n{'='*80}")
    logger.info(f"âœ… å…¨éƒ¨å®Œæˆï¼")
    logger.info(f"{'='*80}")


if __name__ == "__main__":
    main()

