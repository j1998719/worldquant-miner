# AlphaSpire - Ollama ç‰ˆæœ¬ä¿®æ”¹è¯´æ˜

## ğŸ¯ ä¿®æ”¹æ¦‚è§ˆ

æœ¬é¡¹ç›®å·²ä» OpenAI/DeepSeek API æ”¹ä¸ºä½¿ç”¨æœ¬åœ° Ollama gemma3:1b æ¨¡å‹ã€‚

## ğŸ“ ä¿®æ”¹çš„æ–‡ä»¶

### 1. `config.yaml`
**æ”¹åŠ¨ï¼š** ç§»é™¤ OpenAI é…ç½®ï¼Œæ·»åŠ  Ollama é…ç½®

```yaml
# åŸæ¥
openai_base_url: "todo"
openai_api_key: "todo"
openai_model_name: "todo"

# ç°åœ¨
ollama_url: "http://localhost:11434"
ollama_model: "gemma3:1b"
```

### 2. `researcher/generate_template.py`
**æ”¹åŠ¨ï¼š** å®Œå…¨é‡å†™

- âŒ ç§»é™¤ï¼š`langchain_openai.ChatOpenAI`
- âŒ ç§»é™¤ï¼š`langchain.chains.LLMChain`
- âŒ ç§»é™¤ï¼š`langchain.memory.ConversationBufferMemory`
- âœ… æ–°å¢ï¼š`call_ollama_llm()` - ç›´æ¥è°ƒç”¨ Ollama API
- âœ… æ–°å¢ï¼šæ¶ˆæ¯æ ¼å¼è½¬æ¢ï¼ˆLangChain æ ¼å¼ â†’ å­—ç¬¦ä¸²ï¼‰

**å…³é”®å‡½æ•°ï¼š**
```python
def call_ollama_llm(prompt: str, timeout: int = 120) -> str:
    """è°ƒç”¨æœ¬åœ° Ollama æ¨¡å‹"""
    ollama_url = ConfigLoader.get("ollama_url")
    model = ConfigLoader.get("ollama_model")
    
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": 0.2,
            "num_predict": 2000
        }
    }
    
    response = requests.post(f"{ollama_url}/api/generate", json=payload, timeout=timeout)
    return response.json().get('response', '').strip()
```

### 3. `main.py`
**æ”¹åŠ¨ï¼š** è·³è¿‡çˆ¬è™«é˜¶æ®µï¼Œä»å·²æœ‰çš„ helpful_posts å¼€å§‹

- âŒ ç§»é™¤ï¼šçˆ¬è™«è°ƒç”¨ï¼ˆ`scrape_new_posts()` å’Œ `preprocess_all_html_posts()`ï¼‰
- âœ… ä¿ç•™ï¼šAlpha ç ”ç©¶æµç¨‹ï¼ˆä»å¸–å­ç”Ÿæˆæ¨¡æ¿ï¼‰
- âœ… ä¿ç•™ï¼šAlpha è¯„ä¼°æµç¨‹ï¼ˆå›æµ‹ï¼‰

**æ–°æµç¨‹ï¼š**
```
helpful_posts/ (å·²å­˜åœ¨)
    â†“
ç”Ÿæˆå‡è®¾ (Hypothesis)
    â†“
ç”Ÿæˆæ¨¡æ¿ (Template)
    â†“
ç”Ÿæˆ Alpha è¡¨è¾¾å¼
    â†“
å›æµ‹è¯„ä¼°
```

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### å‰ææ¡ä»¶

1. **Ollama å·²å®‰è£…å¹¶è¿è¡Œ**
   ```bash
   # æ£€æŸ¥ Ollama çŠ¶æ€
   ollama list
   
   # ç¡®ä¿ gemma3:1b å·²ä¸‹è½½
   ollama pull gemma3:1b
   ```

2. **å·²æœ‰ helpful_posts æ•°æ®**
   ```bash
   # ç¡®ä¿ç›®å½•å­˜åœ¨ä¸”æœ‰æ•°æ®
   ls data/wq_posts/helpful_posts/
   ```

### è¿è¡Œæµç¨‹

```bash
cd /Users/chiao-yuyang/Desktop/worldquant-miner/AlphaSpire

# è¿è¡Œå®Œæ•´æµç¨‹
python main.py
```

### åˆ†é˜¶æ®µè¿è¡Œï¼ˆå¯é€‰ï¼‰

```bash
# åªè¿è¡Œç ”ç©¶éƒ¨åˆ†ï¼ˆä»å¸–å­ç”Ÿæˆæ¨¡æ¿å’Œ Alphaï¼‰
python main_researcher.py

# åªè¿è¡Œè¯„ä¼°éƒ¨åˆ†ï¼ˆå›æµ‹ï¼‰
python main_evaluator.py
```

## ğŸ“Š æ•°æ®æµ

```
data/
â”œâ”€â”€ wq_posts/
â”‚   â””â”€â”€ helpful_posts/          â† è¾“å…¥ï¼ˆä½ å·²æœ‰ï¼‰
â”‚       â””â”€â”€ *.json
â”‚
â”œâ”€â”€ hypothesis_db_v2/           â† ä¸­é—´äº§ç‰© 1
â”‚   â””â”€â”€ *_hypotheses.json
â”‚
â”œâ”€â”€ template_db_v2/             â† ä¸­é—´äº§ç‰© 2
â”‚   â””â”€â”€ *_template.json
â”‚
â””â”€â”€ alpha_db_v2/
    â””â”€â”€ all_alphas/             â† æœ€ç»ˆäº§ç‰©
        â””â”€â”€ *_alphas.json
```

## ğŸ” å…³é”®å·®å¼‚å¯¹æ¯”

| ç‰¹æ€§ | åŸç‰ˆ (OpenAI) | Ollama ç‰ˆ |
|------|--------------|-----------|
| **LLM æä¾›å•†** | OpenAI/DeepSeek API | æœ¬åœ° Ollama |
| **æ¨¡å‹** | deepseek-chat | gemma3:1b |
| **ä¾èµ–åº“** | langchain_openai | requests |
| **è°ƒç”¨æ–¹å¼** | LangChain Chains | ç›´æ¥ HTTP API |
| **æ¶ˆæ¯æ ¼å¼** | LangChain æ ¼å¼ | å­—ç¬¦ä¸²æ ¼å¼ |
| **Memory** | ConversationBufferMemory | æ— ï¼ˆæ¯æ¬¡ç‹¬ç«‹ï¼‰|
| **è´¹ç”¨** | æŒ‰ API è°ƒç”¨ä»˜è´¹ | å®Œå…¨å…è´¹ |
| **é€Ÿåº¦** | å–å†³äºç½‘ç»œ | æœ¬åœ°æ¨ç† |

## âš™ï¸ é…ç½®å‚æ•°

åœ¨ `config.yaml` ä¸­å¯è°ƒæ•´ï¼š

```yaml
# Ollama è®¾ç½®
ollama_url: "http://localhost:11434"  # Ollama API åœ°å€
ollama_model: "gemma3:1b"              # ä½¿ç”¨çš„æ¨¡å‹

# å¯é€‰å…¶ä»–æ¨¡å‹
# ollama_model: "qwen3-vl:4b"
# ollama_model: "deepseek-r1:8b"
```

åœ¨ `generate_template.py` ä¸­å¯è°ƒæ•´ LLM å‚æ•°ï¼š

```python
payload = {
    "model": model,
    "prompt": prompt,
    "stream": False,
    "options": {
        "temperature": 0.2,      # æ¸©åº¦ï¼ˆ0-1ï¼Œè¶Šä½è¶Šç¡®å®šï¼‰
        "num_predict": 2000      # æœ€å¤§è¾“å‡º token æ•°
    }
}
```

## ğŸ› å¸¸è§é—®é¢˜

### 1. Ollama è¿æ¥å¤±è´¥
```
âŒ Ollama API é”™è¯¯: Connection refused
```
**è§£å†³ï¼š** ç¡®ä¿ Ollama æœåŠ¡è¿è¡Œä¸­
```bash
# æ£€æŸ¥ Ollama çŠ¶æ€
curl http://localhost:11434/api/version

# å¦‚æœæ²¡è¿è¡Œï¼Œå¯åŠ¨ Ollama
ollama serve
```

### 2. æ¨¡å‹æœªæ‰¾åˆ°
```
âŒ model 'gemma3:1b' not found
```
**è§£å†³ï¼š** ä¸‹è½½æ¨¡å‹
```bash
ollama pull gemma3:1b
```

### 3. JSON è§£æå¤±è´¥
```
âŒ Hypotheses output not valid JSON
```
**åŸå› ï¼š** Ollama è¿”å›çš„æ ¼å¼ä¸æ˜¯çº¯ JSON
**è§£å†³ï¼š** ä»£ç å·²åŒ…å« JSON æå–é€»è¾‘ï¼ˆ`extract_json()`ï¼‰ï¼Œä¼šè‡ªåŠ¨å¤„ç†

### 4. å“åº”æ—¶é—´è¿‡é•¿
**è§£å†³ï¼š** å¯ä»¥ï¼š
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆå¦‚ `gemma3:1b` è€Œä¸æ˜¯ `qwen3-vl:4b`ï¼‰
- å‡å°‘ `num_predict` å‚æ•°
- ä½¿ç”¨ GPU åŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”

åŸºäºæµ‹è¯•æ•°æ®ï¼š

| æ¨¡å‹ | å¹³å‡å“åº”æ—¶é—´ | å†…å­˜å ç”¨ | è´¨é‡ |
|------|-------------|---------|------|
| gemma3:1b | ~3-5ç§’ | ~1.5GB | ä¸­ç­‰ |
| qwen3-vl:4b | ~20-60ç§’ | ~5GB | è¾ƒå¥½ |
| deepseek-r1:8b | ~40-120ç§’ | ~10GB | æœ€å¥½ |

**æ¨èï¼š** å¯¹äºå¿«é€Ÿè¿­ä»£ï¼Œä½¿ç”¨ `gemma3:1b`

## ğŸ¯ ä¸‹ä¸€æ­¥

1. âœ… è¿è¡Œ `main.py` æµ‹è¯•å®Œæ•´æµç¨‹
2. âœ… æ£€æŸ¥ç”Ÿæˆçš„æ¨¡æ¿è´¨é‡
3. âœ… è°ƒæ•´ Ollama å‚æ•°ä¼˜åŒ–ç»“æœ
4. âœ… æ‰¹é‡å¤„ç†æ›´å¤šå¸–å­

## ğŸ“š å‚è€ƒ

- [Ollama æ–‡æ¡£](https://github.com/ollama/ollama)
- [Ollama API æ–‡æ¡£](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [AlphaSpire åŸé¡¹ç›®](https://github.com/Argithun/AlphaSpire)

