import random
import json
import requests
import time
from pathlib import Path

from researcher.construct_prompts import (
    build_wq_knowledge_prompt, 
    build_blog_to_hypothesis, 
    build_hypothesis_to_template, 
    build_check_if_blog_helpful
)
from utils.config_loader import ConfigLoader
from utils.json_dealer import extract_json

# --- è·¯å¾„ ---
BASE_DIR = Path(__file__).resolve().parents[1]
POSTS_DIR = BASE_DIR / "data" / "wq_posts" / "helpful_posts"
HYPOTHESIS_DB = BASE_DIR / "data" / "hypothesis_db_v2"
TEMPLATE_DB = BASE_DIR / "data" / "template_db_v2"

HYPOTHESIS_DB.mkdir(parents=True, exist_ok=True)
TEMPLATE_DB.mkdir(parents=True, exist_ok=True)


# === Ollama LLM è°ƒç”¨ ===
def call_ollama_llm(prompt: str, timeout: int = 120) -> str:
    """
    è°ƒç”¨æœ¬åœ° Ollama æ¨¡å‹ï¼ˆå‚è€ƒ adaptive_alpha_miner.pyï¼‰
    
    Args:
        prompt: æç¤ºè¯ï¼ˆåŒ…å« system + user å†…å®¹ï¼‰
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    
    Returns:
        æ¨¡å‹çš„å“åº”æ–‡æœ¬
    """
    ollama_url = ConfigLoader.get("ollama_url")
    model = ConfigLoader.get("ollama_model")
    
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": 0.2,
            "num_predict": 2000
        }
    }
    
    try:
        print(f"ğŸ¤– è°ƒç”¨ Ollama æ¨¡å‹: {model}")
        start_time = time.time()
        
        response = requests.post(
            f"{ollama_url}/api/generate", 
            json=payload, 
            timeout=timeout
        )
        
        elapsed_time = time.time() - start_time
        print(f"â±ï¸  Ollama å“åº”æ—¶é—´: {elapsed_time:.2f}s")
        
        if response.status_code == 200:
            result = response.json()
            response_text = result.get('response', '').strip()
            return response_text
        else:
            print(f"âŒ Ollama API é”™è¯¯: {response.status_code}")
            print(f"   {response.text[:200]}")
            return ""
            
    except requests.exceptions.Timeout:
        print(f"â±ï¸  Ollama è¯·æ±‚è¶…æ—¶ï¼ˆ{timeout}sï¼‰")
        return ""
    except Exception as e:
        print(f"âŒ è°ƒç”¨ Ollama å¤±è´¥: {e}")
        return ""


def init_system_prompt():
    """åˆå§‹åŒ–ç³»ç»Ÿæç¤ºè¯"""
    return build_wq_knowledge_prompt()


# === éšæœºé€‰æ‹©æœ‰ç”¨çš„ Blog Post ===
def select_valid_post():
    post_files = list(POSTS_DIR.glob("*.json"))
    if not post_files:
        raise FileNotFoundError("âŒ No blog post found in processed_posts folder")
    return random.choice(post_files)


def check_if_post_helpful(system_prompt: str, post_file) -> bool:
    """æ£€æŸ¥å¸–å­æ˜¯å¦æœ‰å¸®åŠ©"""
    # æ„å»ºç”¨æˆ· promptï¼ˆå·²ç»æ˜¯å­—ç¬¦ä¸²äº†ï¼‰
    user_prompt = build_check_if_blog_helpful(post_file)
    
    # æ„å»ºå®Œæ•´ prompt
    full_prompt = f"System: {system_prompt}\n\nUser: {user_prompt}"
    
    # è°ƒç”¨ Ollama
    output = call_ollama_llm(full_prompt)
    
    return output.upper().startswith("Y") if output else False


# === ç”Ÿæˆ Hypotheses ===
def generate_hypotheses(system_prompt: str, post_file):
    """ä»å¸–å­ç”Ÿæˆå‡è®¾"""
    user_prompt = build_blog_to_hypothesis(post_file)
    
    # æ„å»ºå®Œæ•´ promptï¼ˆå·²ç»æ˜¯å­—ç¬¦ä¸²äº†ï¼‰
    full_prompt = f"System: {system_prompt}\n\nUser: {user_prompt}"
    
    # è°ƒç”¨ Ollama
    output = call_ollama_llm(full_prompt)
    
    if not output:
        raise ValueError(f"âŒ Ollama è¿”å›ç©ºå“åº”")

    try:
        hypotheses = extract_json(output)
    except Exception as e:
        print(f"âŒ Hypotheses output not valid JSON: {output}")
        raise ValueError(f"âŒ Failed to extract JSON: {e}")

    out_file = HYPOTHESIS_DB / f"{Path(post_file).stem}_hypotheses.json"
    with open(out_file, "w", encoding="utf-8") as f:
        json.dump(hypotheses, f, indent=2, ensure_ascii=False)

    print(f"âœ… Hypotheses saved: {out_file}")
    return out_file


# === ç”Ÿæˆ Template ===
def generate_template(system_prompt: str, hypotheses_file):
    """ä»å‡è®¾ç”Ÿæˆæ¨¡æ¿"""
    user_prompt = build_hypothesis_to_template(hypotheses_file)
    
    # æ„å»ºå®Œæ•´ promptï¼ˆå·²ç»æ˜¯å­—ç¬¦ä¸²äº†ï¼‰
    full_prompt = f"System: {system_prompt}\n\nUser: {user_prompt}"
    
    # è°ƒç”¨ Ollama
    output = call_ollama_llm(full_prompt)
    
    if not output:
        print(f"âŒ Ollama è¿”å›ç©ºå“åº”")
        return None

    try:
        template_json = extract_json(output)
    except Exception:
        print(f"âŒ Template output not valid JSON: {output}")
        return None

    out_file = TEMPLATE_DB / f"{Path(hypotheses_file).stem}_template.json"
    with open(out_file, "w", encoding="utf-8") as f:
        json.dump(template_json, f, indent=2, ensure_ascii=False)

    print(f"âœ… Template saved: {out_file}")
    return out_file


# === ä¸»æµç¨‹ ===
def from_post_to_template(post_file: str = None):
    """
    ä»å¸–å­åˆ°æ¨¡æ¿çš„å®Œæ•´æµç¨‹
    
    Args:
        post_file: å¸–å­æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸º None åˆ™éšæœºé€‰æ‹©
    
    Returns:
        æ¨¡æ¿æ–‡ä»¶è·¯å¾„æˆ– None
    """
    # åˆå§‹åŒ–ç³»ç»Ÿæç¤ºè¯
    system_prompt = init_system_prompt()

    # Step 1: é€‰æ‹© blog
    if post_file:
        post_stem = Path(post_file).stem
        existing_template = TEMPLATE_DB / f"{post_stem}_hypotheses_template.json"
        if existing_template.exists():
            print(f"âœ… Template already exists for {post_file}, skipping.")
            return None
        blog_file = post_file
    else:
        blog_file = select_valid_post()

    print(f"\n{'='*60}")
    print(f"ğŸ“„ å¤„ç†å¸–å­: {blog_file}")
    print(f"{'='*60}")

    # Step 2: ç”Ÿæˆ Hypotheses
    try:
        hypotheses_file = generate_hypotheses(system_prompt, blog_file)
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå‡è®¾å¤±è´¥: {e}")
        return None

    # Step 3: ç”Ÿæˆ Template
    try:
        template_file = generate_template(system_prompt, hypotheses_file)
    except Exception as e:
        print(f"âŒ ç”Ÿæˆæ¨¡æ¿å¤±è´¥: {e}")
        return None

    if template_file:
        print(f"ğŸ¯ å®Œæˆ: ä» {blog_file} æˆåŠŸç”Ÿæˆæ¨¡æ¿")
    
    return template_file


if __name__ == "__main__":
    from_post_to_template()
