import os
import json
import unicodedata
from pathlib import Path
from bs4 import BeautifulSoup
from langchain_openai import ChatOpenAI
from loguru import logger

from researcher.construct_prompts import build_check_if_blog_helpful
from utils.config_loader import ConfigLoader

BASE_DIR = Path(__file__).resolve().parents[1]
RAW_DIR = BASE_DIR / "data" / "wq_posts" / "raw_posts"
RAW_DIR.mkdir(parents=True, exist_ok=True)
PROCESSED_DIR = BASE_DIR / "data" / "wq_posts" / "processed_posts"
PROCESSED_DIR.mkdir(parents=True, exist_ok=True)
HELPFUL_DIR = BASE_DIR / "data" / "wq_posts" / "helpful_posts"
HELPFUL_DIR.mkdir(parents=True, exist_ok=True)


def clean_text(text: str) -> str:
    """
    æ¸…æ´—æ–‡æœ¬ä¸­éutf-8å­—ç¬¦ï¼Œç»Ÿä¸€ä¸º NFC æ ¼å¼
    """
    if not text:
        return ""
    # å…ˆ encode/decode ä¸¢å¼ƒéæ³•å­—ç¬¦
    cleaned = text.encode("utf-8", errors="ignore").decode("utf-8", errors="ignore")
    # å†åš Unicode å½’ä¸€åŒ–ï¼Œé¿å…å¥‡æ€ªçš„å˜ä½“
    cleaned = unicodedata.normalize("NFC", cleaned)
    return cleaned.strip()

def extract_post_info(html_content: str) -> dict:
    """ä»å•ä¸ªHTMLä¸­æŠ½å– description, title, post-body, post-comments"""
    soup = BeautifulSoup(html_content, "html.parser")

    # description
    description = ""
    meta_desc = soup.find("meta", attrs={"name": "description"})
    if meta_desc and meta_desc.get("content"):
        description = meta_desc.get("content").strip()
    if not description:  # å¤‡ç”¨
        og_desc = soup.find("meta", property="og:description")
        if og_desc and og_desc.get("content"):
            description = og_desc.get("content").strip()

    # title
    title = ""
    og_title = soup.find("meta", property="og:title")
    if og_title and og_title.get("content"):
        title = og_title.get("content").strip()
    if not title and soup.title:
        title = soup.title.string.strip()

    # post-body
    post_body = ""
    body_div = soup.find("div", class_="post-body")
    if body_div:
        post_body = body_div.get_text("\n", strip=True)

    # commentsï¼ˆsection.comment-bodyï¼‰
    comments = []
    for section in soup.select("section.comment-body"):
        text = section.get_text("\n", strip=True)
        if text:
            comments.append(text)

    return {
        "title": title,
        "description": description,
        "post_body": post_body,
        "post_comments": comments,
    }

def preprocess_all_html_posts() -> None:
    """æ‰¹é‡å¤„ç†RAW_DIRä¸‹æ‰€æœ‰æœªå¤„ç†çš„htmlæ–‡ä»¶"""
    raw_files = list(RAW_DIR.glob("*.html"))
    logger.info(f"Found {len(raw_files)} raw html files.")
    processed_count = 0

    for raw_file in raw_files:
        post_id = raw_file.stem  # æ–‡ä»¶åä¸å¸¦åç¼€
        out_file = PROCESSED_DIR / f"{post_id}.json"
        if out_file.exists():
            continue  # å·²å¤„ç†

        logger.info(f"Processing {raw_file.name}...")
        html_content = raw_file.read_text(encoding="utf-8", errors="ignore")

        # å†åšä¸€å±‚cleanï¼Œé¿å…HTMLå†…çš„éæ³•å­—ç¬¦
        html_content = clean_text(html_content)

        post_info = extract_post_info(html_content)

        with open(out_file, "w", encoding="utf-8") as f:
            json.dump(post_info, f, ensure_ascii=False, indent=2)

        processed_count += 1
        logger.info(f"Saved processed JSON to {out_file}")

        if check_if_post_helpful(out_file):
            helpful_file = HELPFUL_DIR / f"{post_id}.json"
            with open(helpful_file, "w", encoding="utf-8") as f:
                json.dump(post_info, f, ensure_ascii=False, indent=2)

    logger.info(f"Total processed new files: {processed_count}")


def check_if_post_helpful(post_file):
    base_url = ConfigLoader.get("openai_base_url")
    api_key = ConfigLoader.get("openai_api_key")
    model_name = ConfigLoader.get("openai_model_name")

    llm = ChatOpenAI(
        base_url=base_url,
        api_key=api_key,
        model=model_name,
        temperature=0.2,
    )
    formatted = build_check_if_blog_helpful(post_file)

    try:
        resp = llm.invoke(formatted)
        if hasattr(resp, "content"):
            answer = resp.content.strip()
        else:
            answer = str(resp).strip()

        print(f"ğŸ” Model output for if {post_file} helpful: {answer}")

        if answer.upper().startswith("Y"):
            return True
        else:
            return False
    except Exception as e:
        print(f"âš ï¸ check_if_post_helpful error: {e}")
        return False


if __name__ == "__main__":
    preprocess_all_html_posts()